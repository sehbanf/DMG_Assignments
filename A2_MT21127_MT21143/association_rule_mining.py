# -*- coding: utf-8 -*-
"""Association_Rule_Mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_G1hiAu7Vpvf_nTf507bJxoo84K_lnvq

Ans-1
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split

df1 = pd.read_csv("links.csv")
df2 = pd.read_csv("movies.csv")
df3 = pd.read_csv("ratings.csv")
df4 = pd.read_csv("tags.csv")

#Finding null values in links csv file
print('Number of null values in links files:\n',df1.isna().sum(),sep="")
df1.loc[df1.tmdbId.isna()]

#Finding null values in movies, ratings and tags csv file respectively
print('For movies csv file:\n',df2.isna().sum())
print('For rating csv file:\n',df3.isna().sum())
print('For tags csv file:\n',df4.isna().sum())

#Finding frequent values in categorical attributes
df2.title.value_counts().sort_values(ascending=False)
df2.genres.value_counts().sort_values(ascending=False)
df4.tag.value_counts().sort_values(ascending=False)

df3['userId'].nunique()

merge_MR = pd.merge(df2,df3,on='movieId',how= 'outer')

merge_MR.info()

test_df = merge_MR.drop('timestamp',axis = 1)

test_df.info()

test_df.isnull().sum()

test_df.head()

test = test_df.dropna(how='any',axis=0)

test.isnull().sum()

test.info()

test1 = test.drop_duplicates(['userId','title'])

test1.info()

final_df = test1.pivot(index='userId', columns='title', values='rating').fillna(0)

plt.figure(figsize=(10,5))
ax = sns.countplot(data=test1, x='rating')
labels = (test1['rating'].value_counts().sort_index())
plt.title('Distribution of Ratings')
plt.xlabel('Ratings')

for i,v in enumerate(labels):
    ax.text(i, v+100, str(v), horizontalalignment='center', size=14, color='black')
plt.show()

correlations = test1.corr()
print(correlations)
sns.heatmap(correlations,annot=True,cmap="YlGnBu")
plt.show()

"""Ans - 2"""

final_df = final_df.astype('int64')

def encode_ratings(x):
    if x<=0:
        return 0
    if x>=1:
        return 1

final_df = final_df.applymap(encode_ratings)

from mlxtend.frequent_patterns import apriori

frequent_itemset = apriori(final_df, min_support=0.1, use_colnames=True)

from mlxtend.frequent_patterns import association_rules

rules = association_rules(frequent_itemset, metric="lift", min_threshold=1)

df_rules = rules.sort_values(by=['lift'], ascending=False)
df_rules

#df_MIB = df_res[df_res['antecedents'].apply(lambda x: len(x) ==1 and next(iter(x)) == 'Forrest Gump (1994)')]
movies_watched =  ['Matrix, The (1999)']
df_movies = df_rules[df_rules['antecedents'].apply(lambda x: set(movies_watched) == set(x))]

df_movies = df_movies[df_movies['lift'] > 2]

df_movies

movies = df_movies['consequents'].values

movie_list = []
for movie in movies:
    for title in movie:
        if title not in movie_list:
            movie_list.append(title)

movie_list[0:4]



"""Ans - 3"""

su = frequent_itemset.support.unique()

fredic = {}
for i in range(len(su)):
    inset = list(frequent_itemset.loc[frequent_itemset.support ==su[i]]['itemsets'])
    fredic[su[i]] = inset
#Dictionay storing itemset with  support count <= key
fredic2 = {}
for i in range(len(su)):
    inset2 = list(frequent_itemset.loc[frequent_itemset.support<=su[i]]['itemsets'])
    fredic2[su[i]] = inset2

ml = []
for index, row in frequent_itemset.iterrows():
    isclose = True
    cli = row['itemsets']
    cls = row['support']
    checkset = fredic2[cls]
    for i in checkset:
        if (cli!=i):
            if(frozenset.issubset(cli,i)):
                isclose = False
                break
    
    if(isclose):
        ml.append(row['itemsets'])

ml

